---
layout: post
title:  "3. 자바를 활용한 스파크 스트리밍"
date:   2019-03-17
desc: "자바를 활용한 스파크 스트리밍"
keywords: "스파크 스트리밍,spark streaming,java spark"
categories: [Spark]
tags: [spark]
icon: icon-html
---
# __3. 자바를 활용한 스파크 스트리밍__
---

## __0. 스파크 스트리밍이란?__
---
### 배경


빅데이터의 시대가 다가오면서 데이터의 크기(Volume)과 함께 데이터의 처리 속도(Velocity) 또한 중요한 요소로 자리잡게 됐다.

이에 따라 데이터를 모아뒀다가 정해진 시간마다 처리하는 기존의 방식이 아닌

실시간으로 데이터를 처리해주는 기술이 중요해졌다.

스파크 스트리밍(Spark Streaming)은 이런 배경에서 나온 기술이라고 할 수 있다.

스파크 스트리밍은 실시간으로 데이터를 받아와서 실시간으로 스트리밍 처리를 할 수 있게 도와준다.

---
### 스파크 스트리밍

![image](/images/streaming-flow.png)

스파크 스트리밍은 입력 데이터를 받아서

내부적으로 데이터를 여러개의 작은 배치로 나눈다.

그리고 스파크 엔진이 배치 입력 데이터를 처리하는 구조이다.

또 하나 알아 둘 중요한 점이 있다.

스파크 스트리밍은 DStream으로 추상화 되어있다.

말이 어려울 수 있으니 DStream을 이용한다고 생각하자.

DStream은 RDD를 기반으로 만들어졌는데 (내부적으로 DStream은 RDD로 표현 됨)

이런 특성때문에 개발자들이 RDD 컨텍스트를 사용할 수 있게 됐다.

추가적으로, 스파크 스트리밍은 Kafka, Flume, Kinesis와 연동되기도 한다.

지금까지가 SparkStreaming에 대한 간단한 설명이었다.

이젠 스파크 스트리밍을 활용해보기 위해

간단한 워드 카운트 어플리케이션을 만들어보자.


---
## __1. pom.xml 수정__


이번 과제에선 스파크 스트리밍을 활용해야 하기 때문에

기존 pom.xml 파일에 dependency를 추가해야 한다.
~~~
<dependency>
    	<groupId>org.apache.spark</groupId>
    	<artifactId>spark-streaming_2.11</artifactId>
    	<version>2.4.0</version>
</dependency>
~~~
dependency를 추가함으로서 spark streaming을 위한 추가적인 라이브러리를 사용할 수 있다. 


---
## __2. 라이브러리 설정__


로컬 SparkContext와 Streaming Context를 활용하기 위해 해당 라이브러리를 추가한다.
~~~
import org.apache.spark.*;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;
~~~
추가적으로, 뒤에 사용 될 함수를 위해 다음 라이브러리도 추가한다.
~~~
import scala.Tuple2;
import java.util.Arrays;
~~~


---
## __3. 스레드 생성 및 배치 설정__


두개의 스레드를 가진 로컬 Streaming Context를 생성하고

배치의 간격을 10초로 설정한다. 
~~~
SparkConf conf = new SparkConf().setAppName("ex4").setMaster("local[2]");
JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(10));
~~~


---
## __4. DStream 생성__


로컬호스트 9999포트로 접근하는 DStream을 생성해보자. 

jssc.socketTextStream 함수는 특정 소켓의 텍스트 스트림을 보기 위한 스파크 스트리밍 함수이다.

아래 코드를 통해 로컬 PC의 9999 포트에서 Input text를 가져와 DStream에 저장한다.
~~~
JavaReceiverInputDStream<String> lines = jssc.socketTextStream("localhost", 9999);
~~~

DStream에는 아까 설정했던 batch 간격으로 받아온 RDD들이 다음과 같이 연속적으로 저장되어 있다.

![image](/images/streaming-dstream.png)

---
## __5. 데이터 처리__


이젠 위 과정을 통해 받은 데이터를 처리해보자. 

DStream에서 지원하는 flatmap 함수를 통해 lines에 저장된 RDD를 단어 단위로 끊어서 words에 저장한다.

batch 간격으로 나누어진 RDD들이 연산을 통해 새로운 RDD를 생성한다.

변화 과정은 다음과 같다.

![image](/images/streaming-dstream-ops.png)

~~~
JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(x.split(" ")).iterator());
~~~
데이터를 나눈뒤 각 배치(10초)에 포함된 단어의 개수를 센다.

단어의 개수를 셀 때 DStream에서 지원하는 mapToPair,reduceByKey 함수를 사용한다.
~~~
JavaPairDStream<String, Integer> pairs = words.mapToPair(s -> new Tuple2<>(s, 1));
JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey((i1, i2) -> i1 + i2);
~~~
결과를 확인하기 위해 콘솔창에 각 RDD에 있는 10개의 데이터를 출력한다.
~~~
wordCounts.print();
~~~


---
## __6. 스트리밍 연산__


가장 중요한 스트리밍 연산을 하기 위해 아래 코드를 추가한다.
~~~
jssc.start();
jssc.awaitTermination();
~~~
jssc.start() 함수는 스트리밍 연산을 시작하는 것이고,

jssc.awaitTermination() 함수는 스트리밍 연산의 종료를 기다리는 함수이다. 

연산이 종료되지 않으면 스트리밍은 멈추지 않고 계속 스트리밍을 읽기 위해 대기한다.


---
## __7. .jar 파일 생성__


코드가 완성되었으면 .jar 파일을 생성한다.

완성된 코드는 다음과 같다.
~~~
package spark1;

import org.apache.spark.*;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;

import scala.Tuple2;
import java.util.Arrays;

public final class JavaWordCount {
public static void main(String[] args) throws Exception {
	  SparkConf conf = new SparkConf().setAppName("ex4").setMaster("local[2]");
	  JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(10));
	  JavaReceiverInputDStream<String> lines = jssc.socketTextStream("localhost", 9999);
	  JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(x.split(" ")).iterator());
	  
	  JavaPairDStream<String, Integer> pairs = words.mapToPair(s -> new Tuple2<>(s, 1));
	  JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey((i1, i2) -> i1 + i2);
	  wordCounts.print();
	  
	  jssc.start();              // Start the computation
	  jssc.awaitTermination();   // Wait for the computation to terminate
  }
}
~~~
해당 코드를 빌드한 뒤, 자바 어플리케이션 파일을 생성한다.


---
## __8. 스파크 어플리케이션 실행__


빌드가 끝났다면 스파크 어플리케이션을 실행해보자. 

스파크가 설치된 디렉토리로 이동한 뒤 다음 명령어를 통해 스파크 어플리케이션을 실행한다.
~~~
bin/spark-submit --class <패키지 이름>.<클래스 이름> <어플리케이션 이름>.jar localhost 9999
~~~


---
## __9. 데이터 전송__


어플리케이션이 실행되고 있다면 새로운 터미널 창을 연 뒤

데이터를 9999포트로 전송해보자. 

데이터를 전송하기 위해 터미널에 다음 명령어를 수행한다.
~~~
nc -lk 9999
~~~
그리고 아무 문자나 입력한 뒤 엔터를 눌러 전송한다.
~~~
happy new year cheer up baby
~~~
![image](/images/nc_lk.png)


---
## __10. 결과 확인__


스파크 스트리밍이 제대로 구현됐다면 다음과 같은 결과를 얻을 수 있다. 


![image](/images/output.png)


---
