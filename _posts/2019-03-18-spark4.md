---
layout: post
title:  "4. 자바를 활용한 구조적 스트리밍"
date:   2019-03-17
desc: "자바를 활용한 구조적 스트리밍"
keywords: "스파크 스트리밍,structured streaming,java spark"
categories: [Spark]
tags: [spark]
icon: icon-html
---
# __4. 자바를 활용한 구조적 스트리밍__
---
## __0. 스파크 스트리밍이란?__
---

### 구조적 스트리밍

구조적 스트리밍은 스파크의 구조적 API(데이터 프레임, 데이터 셋, SQL)을 통한 스트리밍 방법이다.

RDD를 이용했던 스파크 스트리밍의 DSTREAM을 이용한 방법과 다르다는 것을 기억하자.

구조적 스트리밍의 모든 쿼리는 카탈리스트 쿼리 옵티마이저를 거치므로

사용자는 실시간 스트리밍 데이터를 대상으로 SQL 쿼리를 수행할 수 있다.

![image](/images/structured-streaming-table.png){: width="981" height="528"}

추가적으로 구조적 스트리밍의 핵심 아이디어는 입력으로 들어오는 스트리밍 데이터에 대해 테이블 형식으로 추가할 수 있다는 점이다

따라서, 스트리밍으로 들어오는 모든 입력은 테이블에 새로 추가되는 새로운 행과 같다고 보면 된다.

방금 언급했던 "스트리밍 데이터를 대상으로 SQL 쿼리를 수행한다"의 의미는

테이블에 있는 입력 데이터를 대상으로 질의를 할 수 있다는 것과 같은 말이다.

이젠 구조적 스트리밍을 활용하기 위해

입력 데이터의 단어를 세는 어플리케이션을 설계해보자.

---
## __1. 라이브러리 설정__


로컬 SparkSession을 사용하기 위해 해당 라이브러리를 추가한다.
~~~
import org.apache.spark.*;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;
~~~
추가적으로, 뒤에 사용 될 함수를 위해 다음 라이브러리도 추가한다.
~~~
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import java.util.Arrays;
~~~


---
## __2. SparkSession 생성__


로컬 SparkSession를 생성해보자.

이전과 달리 Streaming Context를 생성하지 않는 이유는

SparkSession 내에 포함되어있기 때문이다.
~~~
SparkSession spark = SparkSession
			  .builder()
			  .appName("JavaStructuredNetworkWordCount")
			  .getOrCreate();
~~~


---
## __3. 입력을 받아 데이터 프레임 생성__


SparkStreaming을 이용했을 때와 마찬가지로

입력을 받기 위해 로컬호스트 9999포트를 이용한다.

하지만 더 이상 DStream을 사용하지 않는다.

아래 코드를 통해 로컬 PC의 9999 포트에서 입력을 받아 데이터 프레임을 생성한다.
~~~
Dataset<Row> lines = spark
		  .readStream()
		  .format("socket")
		  .option("host", "localhost")
		  .option("port", 9999)
		  .load();
~~~


---
## __4. 데이터 처리__


위 과정을 통해 받은 데이터를 처리해보자. 

우선 아래 함수를 통해 받아온 인풋을 워드 단위로 나눈다.
~~~
Dataset<String> words = lines
		  .as(Encoders.STRING())
		  .flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());
~~~
데이터를 나눈뒤 단어의 개수를 세 보자.

이 때 SparkStreaming과 구분되는 중요한 차이가 있다.

구조적 스트리밍에선 RDD가 아닌 Dataset을 이용하기 때문에 

mapToPair, reduceByKey 함수를 쓸 필요 없이

groupBy, count 함수를 통해 단어의 개수를 셀 수 있다.
~~~
Dataset<Row> wordCounts = words.groupBy("value").count();
~~~
결과를 확인하기 위해 콘솔에 단어의 갯수를

출력해주는 쿼리를 수행해보자.

구조적 스트리밍에선 Dataset을 사용했기 때문에

RDD를 사용해 단어의 수를 세는

SparkStreaming의 방식과는 차이가 있다.
~~~
StreamingQuery query = wordCounts.writeStream()
		  .outputMode("complete")
		  .format("console")
		  .start();
~~~


---
## __5. 스트리밍 연산__


마지막으로 스트리밍을 종료시키기 위해

다음 코드를 추가한다.
~~~
query.awaitTermination();
~~~


---
## __6. .jar 파일 생성__


코드가 완성되었으면 .jar 파일을 생성한다.

완성된 코드는 다음과 같다.
~~~
package spark1;

import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import java.util.Arrays;

public final class JavaWordCount {
public static void main(String[] args) throws Exception {
		SparkSession spark = SparkSession
			  .builder()
			  .appName("JavaStructuredNetworkWordCount")
			  .getOrCreate();
		// Create DataFrame representing the stream of input lines from connection to localhost:9999
		Dataset<Row> lines = spark
		  .readStream()
		  .format("socket")
		  .option("host", "localhost")
		  .option("port", 9999)
		  .load();

		// Split the lines into words
		Dataset<String> words = lines
		  .as(Encoders.STRING())
		  .flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());

		// Generate running word count
		Dataset<Row> wordCounts = words.groupBy("value").count();
	  
		// Start running the query that prints the running counts to the console
		StreamingQuery query = wordCounts.writeStream()
		  .outputMode("complete")
		  .format("console")
		  .start();

		query.awaitTermination();
  }
}
~~~
해당 코드를 빌드한 뒤, 자바 어플리케이션 파일을 생성한다.

---
## __7. 스파크 어플리케이션 실행__

빌드가 끝났다면 스파크 어플리케이션을 실행해보자. 

스파크가 설치된 디렉토리로 이동한 뒤 다음 명령어를 통해 스파크 어플리케이션을 실행한다.
~~~
bin/spark-submit --class <패키지 이름>.<클래스 이름> <어플리케이션 이름>.jar localhost 9999
~~~


---
## __8. 데이터 전송__


어플리케이션이 실행되고 있다면 새로운 터미널 창을 연 뒤

데이터를 9999포트로 전송해보자. 

데이터를 전송하기 위해 터미널에 다음 명령어를 수행한다.
~~~
nc -lk 9999
~~~
그리고 아무 문자나 입력한 뒤 엔터를 눌러 전송한다.
~~~
happy new year cheer up baby
~~~
![image](/images/nc_lk.png)


---
## __9. 결과 확인__


구조적 스트리밍이 제대로 구현됐다면 다음과 같은 결과를 얻을 수 있다. 

단어가 입력되는데, 배치 단위로 초기화 되지 않고

이전의 결과가 남아있다.

![image](/images/spark4_output.png)

설계한 프로그램의 타임테이블은 다음과 같다.

입력된 데이터가 배치 간격으로 초기화 되지 않고 계속해서 유지된다.

![image](/images/structured-streaming-example.png){: width="706" height="654"}

---
