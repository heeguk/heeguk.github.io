---
layout: post
title:  "4. 자바를 활용한 구조적 스트리밍"
date:   2019-03-17
desc: "자바를 활용한 구조적 스트리밍"
keywords: "스파크 스트리밍,structured streaming,java spark"
categories: [Spark]
tags: [spark]
icon: icon-html
---
# __4. 자바를 활용한 구조적 스트리밍__
---

---
## __1. 라이브러리 설정__


로컬 SparkSession을 사용하기 위해 해당 라이브러리를 추가한다.
~~~
import org.apache.spark.*;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;
~~~
추가적으로, 뒤에 사용 될 함수를 위해 다음 라이브러리도 추가한다.
~~~
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import java.util.Arrays;
~~~


---
## __2. SparkSession 생성__


로컬 SparkSession를 생성해보자.

이때 이전과 달리 Streaming Context를 생성하지 않는 이유는

SparkSession 내에 포함되어있기 때문이다.
~~~
SparkSession spark = SparkSession
			  .builder()
			  .appName("JavaStructuredNetworkWordCount")
			  .getOrCreate();
~~~


---
## __3. 입력을 받아 데이터 프레임 생성__


SparkStreaming을 이용했을 때와 마찬가지로

입력을 받기 위해 로컬호스트 9999포트를 이용한다.

더 이상 DStream을 사용하지 않는다. 잠시 잊어두자.

아래 코드를 통해 로컬 PC의 9999 포트에서 입력을 받아 데이터 프레임을 생성한다.
~~~
Dataset<Row> lines = spark
		  .readStream()
		  .format("socket")
		  .option("host", "localhost")
		  .option("port", 9999)
		  .load();
~~~


---
## __4. 데이터 처리__


위 과정을 통해 받은 데이터를 처리해보자. 

우선 아래 함수를 통해 받아온 인풋을 워드 단위로 나눈다.
~~~
Dataset<String> words = lines
		  .as(Encoders.STRING())
		  .flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());
~~~
데이터를 나눈뒤 단어의 개수를 센다.

이 때 SparkStreaming과 구분되는 중요한 차이가 있다.

이젠 mapToPair, reduceByKey 함수를 쓸 필요 없이

groupBy, count 함수를 통해 단어의 개수를 셀 수 있다.
~~~
Dataset<Row> wordCounts = words.groupBy("value").count();
~~~
결과를 확인하기 위해 콘솔에 단어의 count를

출력해주는 쿼리를 수행해보자.
~~~
StreamingQuery query = wordCounts.writeStream()
		  .outputMode("complete")
		  .format("console")
		  .start();
~~~


---
## __5. 스트리밍 연산__


마지막으로 스트리밍을 종료시키기 위해

다음 함수를 실행한다.
~~~
query.awaitTermination();
~~~


---
## __6. .jar 파일 생성__


코드가 완성되었으면 .jar 파일을 생성한다.

완성된 코드는 다음과 같다.
~~~
package spark1;

import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import java.util.Arrays;

public final class JavaWordCount {
public static void main(String[] args) throws Exception {
		SparkSession spark = SparkSession
			  .builder()
			  .appName("JavaStructuredNetworkWordCount")
			  .getOrCreate();
		// Create DataFrame representing the stream of input lines from connection to localhost:9999
		Dataset<Row> lines = spark
		  .readStream()
		  .format("socket")
		  .option("host", "localhost")
		  .option("port", 9999)
		  .load();

		// Split the lines into words
		Dataset<String> words = lines
		  .as(Encoders.STRING())
		  .flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());

		// Generate running word count
		Dataset<Row> wordCounts = words.groupBy("value").count();
	  
		// Start running the query that prints the running counts to the console
		StreamingQuery query = wordCounts.writeStream()
		  .outputMode("complete")
		  .format("console")
		  .start();

		query.awaitTermination();
  }
}
~~~
해당 코드를 빌드한 뒤, 자바 어플리케이션 파일을 생성한다.

---
## __7. 스파크 어플리케이션 실행__

빌드가 끝났다면 스파크 어플리케이션을 실행해보자. 

스파크가 설치된 디렉토리로 이동한 뒤 다음 명령어를 통해 스파크 어플리케이션을 실행한다.
~~~
bin/spark-submit --class <패키지 이름>.<클래스 이름> <어플리케이션 이름>.jar localhost 9999
~~~


---
## __8. 데이터 전송__


어플리케이션이 실행되고 있다면 새로운 터미널 창을 연 뒤

데이터를 9999포트로 전송해보자. 

데이터를 전송하기 위해 터미널에 다음 명령어를 수행한다.
~~~
nc -lk 9999
~~~
그리고 아무 문자나 입력한 뒤 엔터를 눌러 전송한다.
~~~
happy new year cheer up baby
~~~
![image](/images/nc_lk.png)


---
## __9. 결과 확인__


구조적 스트리밍이 제대로 구현됐다면 다음과 같은 결과를 얻을 수 있다. 

단어가 입력되는데, 배치 단위로 초기화 되지 않고

이전의 결과가 남아있다.

![image](/images/_output.png)


---
